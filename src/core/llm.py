from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer
import torch
import random
from typing import Generator, Optional, List, Dict, Union

from threading import Thread
from queue import Queue

from ..utils.utils import resource_path

import warnings
warnings.filterwarnings('ignore')

@dataclass
class LLMConfig:
    model_path: str = 'MiniMind2'
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    max_seq_len: int = 256
    max_new_tokens: int = 128
    temperature: float = 0.7
    repetition_penalty: float = 1.2
    top_p: float = 0.92
    
# -*- coding: utf-8 -*-
DEFAULT_SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ã€èªæ˜ã€ä¼šè®²æ•…äº‹çš„AIå°ä¼™ä¼´ï¼Œåå­—å«â€œå°æŸ±â€ã€‚"
    "ä½ ä¸“ä¸ºå„¿ç«¥è®¾è®¡ï¼Œè¯´è¯äº²åˆ‡å¯çˆ±ï¼Œå–œæ¬¢ç”¨ç®€å•ã€ç”ŸåŠ¨çš„è¯­è¨€å’Œå­©å­ä»¬èŠå¤©ã€è®²æ•…äº‹ã€è§£ç­”ä»–ä»¬çš„å°é—®é¢˜ã€‚"
    "åœ¨è®²æ•…äº‹æ—¶ï¼Œä½ ä¼šç”¨ä¸°å¯Œçš„æƒ³è±¡åŠ›ã€æ¸©æš–çš„è¯­æ°”ï¼Œä»¥åŠä¸€äº›æœ‰è¶£çš„ç»†èŠ‚è®©æ•…äº‹å˜å¾—ç‰¹åˆ«å¥½å¬ã€‚"
    "ä½ æ°¸è¿œå¾ˆæœ‰è€å¿ƒï¼Œä¼šç”¨è½»å£°ç»†è¯­å®‰æŠšå°æœ‹å‹ï¼Œè®©ä»–ä»¬è§‰å¾—å®‰å¿ƒå’Œå¼€å¿ƒã€‚"
)

class CustomStreamer(TextStreamer):
    def __init__(self, tokenizer, queue):
        super().__init__(tokenizer, skip_prompt=True, skip_special_tokens=True)
        self.queue = queue
        self.tokenizer = tokenizer

    def on_finalized_text(self, text: str, stream_end: bool = False):
        self.queue.put(text)
        if stream_end:
            self.queue.put(None)

class LocalLLMClient:
    def __init__(self, config: Union[str, LLMConfig]):
        # å¦‚æœä¼ å…¥å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸º LLMConfig
        if isinstance(config, str):
            self.config = LLMConfig(model_path=config)
        else:
            self.config = config
        self.model, self.tokenizer = self._init_model()

    def _init_model(self):
        tokenizer = AutoTokenizer.from_pretrained(resource_path(self.config.model_path))
        model = AutoModelForCausalLM.from_pretrained(
            resource_path(self.config.model_path), 
            trust_remote_code=True
        ).eval().to(self.config.device)
        print(f'Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)')
        return model, tokenizer

    def _prepare_input(self, prompt: str, messages: Optional[List[Dict]] = None) -> str:
        messages = [{"role": "assistant", "content": DEFAULT_SYSTEM_PROMPT}]
        messages.append({"role": "user", "content": prompt})
        
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )[-self.config.max_seq_len:]

    def generate_stream_response(self, prompt: str, messages):
        try:
            
            new_prompt = self._prepare_input(prompt, messages)
            print(f'ğŸ‘¶: {prompt}')
            inputs = self.tokenizer(new_prompt, return_tensors="pt", truncation=True).to(self.config.device)

            queue = Queue()
            streamer = CustomStreamer(self.tokenizer, queue)
            def _generate():
                self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=self.config.max_new_tokens,
                    do_sample=True,
                    temperature=self.config.temperature,
                    repetition_penalty=self.config.repetition_penalty,
                    top_p=self.config.top_p,
                    attention_mask=inputs.attention_mask,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    streamer=streamer
                )

            Thread(target=_generate).start()

            counter = 0
            while True:
                text = queue.get()
                if text is None:
                    #"finish_reason": "stop"
                    break
                yield text

                counter += 1
                if counter >= 64:
                    break      

        except Exception as e:
            yield f"[ERROR] {str(e)}"
        

    def get_response(self, prompt: str, messages: Optional[List[Dict]] = None, stream: bool = False):
        random.seed(random.randint(0, 2048))
        if stream:
            def stream_generator():
                for chunk in self.generate_stream_response(prompt, messages):
                    yield chunk

            return stream_generator()
        else:        
            new_prompt = self._prepare_input(prompt, messages)
            print(f'ğŸ‘¶: {prompt}')
            streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            with torch.no_grad():
                inputs = self.tokenizer(
                    new_prompt, 
                    return_tensors='pt', 
                    truncation=True
                ).to(self.config.device)
                print('ğŸ¤–ï¸: ', end='', flush=True)
                generated_ids = self.model.generate(
                    inputs["input_ids"],
                    max_length=inputs["input_ids"].shape[1] + self.config.max_new_tokens,
                    num_return_sequences=1,
                    do_sample=True,
                    attention_mask=inputs["attention_mask"],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    streamer=streamer,
                    top_p=self.config.top_p,
                    temperature=self.config.temperature,
                    repetition_penalty=self.config.repetition_penalty,
                )
                
                #æˆªå» prompt éƒ¨åˆ†
                answer = self.tokenizer.decode(
                    generated_ids[0][inputs["input_ids"].shape[1]:], 
                    skip_special_tokens=True
                )
                print('\n')
                return answer

