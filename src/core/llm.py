import torch
import sys
import os
import random
from transformers import AutoTokenizer, AutoModelForCausalLM
from model.model import MiniMindLM
from model.LMConfig import LMConfig
from model.model_lora import apply_lora, load_lora
from argparse import Namespace
from model.model_lora import *


def setup_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def resource_path(path: str) -> str:
    """
    è¿”å›è¿è¡Œæ—¶å¯ä»¥è®¿é—®åˆ°çš„ç»å¯¹è·¯å¾„ï¼š
    1) å¦‚æœç”¨æˆ·ä¼ å…¥çš„æ˜¯ç»å¯¹è·¯å¾„ï¼Œå°±ç›´æ¥è¿”å›ï¼›
    2) å¦åˆ™åœ¨æ‰“åŒ…åï¼Œä» sys._MEIPASS é‡Œæ‰¾ï¼ˆPyInstaller onefileï¼‰ï¼›
    3) å¹³æ—¶å¼€å‘ç¯å¢ƒï¼Œå°±ä»å½“å‰å·¥ä½œç›®å½•æ‰¾ï¼ˆos.path.abspath(".")ï¼‰ã€‚
    """
    # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
    if os.path.isabs(path):
        return path

    # æ‰“åŒ…è¿è¡Œæ—¶ï¼ŒPyInstaller ä¼šæŠŠæ‰€æœ‰èµ„æºè§£å‹åˆ°è¿™é‡Œ
    base_path = getattr(sys, "_MEIPASS", None) or os.path.abspath(".")
    return os.path.join(base_path, path)

class LocalLLMClient:
    def __init__(self, args):
        args = Namespace(
            load=1,
            use_moe=False,
            model_mode=2,
            dim=512,
            n_layers=8,
            max_seq_len=128,
            lora_name='None',
            out_dir='output',
            device='cuda' if torch.cuda.is_available() else 'cpu',
            max_new_tokens=64
        )
        self.model, self.tokenizer = self._init_model(args)
        self.model_mode = args.model_mode
        self.max_seq_len = args.max_seq_len
        self.temperature = 1
        self.top_p = 0.85
        self.device ='cuda' if torch.cuda.is_available() else 'cpu',
        self.stream = False
        self.max_new_tokens = args.max_new_tokens if hasattr(args, 'max_new_tokens') else 128

    def _init_model(self, args):
        real_path = resource_path('model/minimind_tokenizer')
        tokenizer = AutoTokenizer.from_pretrained(real_path)
        if args.load == 0:
            moe_path = '_moe' if args.use_moe else ''
            modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason', 4: 'grpo'} #4 RLAIF
            ckp = f'./{args.out_dir}/{modes[args.model_mode]}_{args.dim}{moe_path}.pth'

            model = MiniMindLM(LMConfig(
                dim=args.dim,
                n_layers=args.n_layers,
                max_seq_len=args.max_seq_len,
                use_moe=args.use_moe
            ))

            state_dict = torch.load(ckp, map_location=args.device)
            model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True)

            if args.lora_name != 'None':
                apply_lora(model)
                load_lora(model, f'./{args.out_dir}/lora/{args.lora_name}_{args.dim}.pth')
        else:
            transformers_model_path = resource_path('MiniMind2-Small')
            tokenizer = AutoTokenizer.from_pretrained(transformers_model_path)
            model = AutoModelForCausalLM.from_pretrained(transformers_model_path, trust_remote_code=True)

        print(f'MiniMindæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M')
        return model.eval().to(args.device), tokenizer

    def get_response(self, prompt: str, messages=None) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒchatæ¨¡æ¿å’Œæµå¼è¾“å‡ºï¼‰"""
        setup_seed(random.randint(0, 2048))

        if messages is None:
            messages = [{
                "role": "system",
                "content": "ä½ æ˜¯å°æŸ±å­ï¼Œä¸€ä¸ªæ¸©æŸ”ã€èªæ˜ã€ä¼šè®²æ•…äº‹çš„AIå°ä¼™ä¼´ï¼Œä¸“ä¸ºå„¿ç«¥è®¾è®¡ã€‚ã€‚"
            }]
        messages.append({"role": "user", "content": prompt})

        # æ„é€  prompt
        if self.model_mode != 0:
            new_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            new_prompt = new_prompt[-self.max_seq_len:]  # ç®€åŒ–è£å‰ªé€»è¾‘
        else:
            new_prompt = self.tokenizer.bos_token + prompt

        print(f'ğŸ‘¶: {prompt}')
        answer = ""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        with torch.no_grad():
            inputs = self.tokenizer(new_prompt, return_tensors='pt')
            input_ids = inputs['input_ids'].to(device=device, dtype=torch.long)

            # å¤„ç† pad_token_id å¯èƒ½ä¸º None
            pad_token_id = self.tokenizer.pad_token_id
            if pad_token_id is None:
                pad_token_id = self.tokenizer.eos_token_id

            # åˆ¤æ–­æ˜¯å¦æ”¯æŒæµå¼
            if not hasattr(self.model, "generate"):
                raise NotImplementedError("å½“å‰æ¨¡å‹ä¸æ”¯æŒ generate æ–¹æ³•")

            # ç”Ÿæˆ
            print('ğŸ¤–ï¸: ', end='', flush=True)
            res_y = self.model.generate(
                input_ids,
                eos_token_id=self.tokenizer.eos_token_id,
                max_new_tokens=self.max_seq_len,
                temperature=self.temperature,
                top_p=self.top_p,
                do_sample=True,
                pad_token_id=pad_token_id
            )

            output_ids = res_y[0][input_ids.shape[1]:]
            answer = self.tokenizer.decode(output_ids, skip_special_tokens=True)
            print(answer.strip())

        return answer.strip()


